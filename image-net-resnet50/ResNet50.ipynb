{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c324bc7c",
   "metadata": {},
   "source": [
    "# Residual Netwworks\n",
    "\n",
    "1. <u>ResNets (2015)</u> is an architecture that has taken over both Language and Image Tasks ever since its inception in 2015. The original papers showed great improvements in the on Image-net and Cifar Datasets by using **Residaul Blocks** to build very very deep networks. Resnets took the depth from previous state of the art of ~20 to 150+\n",
    "\n",
    "\n",
    "##  Benefits\n",
    "1. Solved for **training error degraration** (NOT due to overfitting) often seen in deep learning networks. Note: This is diff from the vanishing gradient problem and overfitting problem usually seen.\n",
    "    * Notice how the red line is higher than the green line during training when the network is deeper (56 Layers) vs 20 Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd47a29",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"PreResnetTrainingError.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5068bb19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T03:21:43.756202Z",
     "start_time": "2023-05-30T03:21:43.751436Z"
    },
    "cell_style": "split"
   },
   "source": [
    "<img src=\"ResnetBuildingBlock.png\" width=\"400\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd0746b",
   "metadata": {},
   "source": [
    "## Mathematics of Resnets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d9629",
   "metadata": {},
   "source": [
    "* Instead of forcing the network to learn a complete transformation, we prefer that it learns a residual transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e8060",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T03:24:05.762565Z",
     "start_time": "2023-05-30T03:24:05.759335Z"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0a31503a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T18:45:03.818797Z",
     "start_time": "2023-05-30T18:45:03.815714Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b5cc9f0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T18:45:04.257578Z",
     "start_time": "2023-05-30T18:45:04.251267Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "writer = SummaryWriter('runs/resnet50')\n",
    "log_dir = f\"{os.getcwd()}/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e0f889cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T18:51:53.289628Z",
     "start_time": "2023-05-30T18:45:05.372412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard has started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.12.3 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if lsof -Pi :6006 -sTCP:LISTEN -t >/dev/null ; then\n",
    "    echo \"TensorBoard is already running\"\n",
    "else\n",
    "    tensorboard --logdir=runs --port=6006 &\n",
    "    echo \"TensorBoard has started\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3ee73f94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T18:51:53.493206Z",
     "start_time": "2023-05-30T18:51:53.490395Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc38b7",
   "metadata": {},
   "source": [
    "### Torchvision Transforms\n",
    "* Here we'll use the **scipt mode** of transforms. The script mode provides efficient implementatoin of transforms by making them device compatible (CPU/GPU)\n",
    "* The wrapper using `ScriptedTransform(torch.nn.Module)` is not necessary. However [torch docs](https://pytorch.org/vision/stable/transforms.html) states that <i>`\"For any custom transformations to be used with torch.jit.script, they should be derived from torch.nn.Module.\"`</i> \n",
    "* Additionally, `torchvision.transforms.Compose()` is in-compatible with `torch.jit.script()` and you must use `torch.nn.Sequential`.\n",
    "* In general, tt's good practice to build modules like this to provide **Compatibility** and **Parameterization**. A common example is having learnable params in in data transformataion pipelines. For now, we don't have any params as such.  \n",
    "* `torch.jit.script()` **does not** support `torchvision.transforms.ToTensor()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a3db93e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T18:51:53.654779Z",
     "start_time": "2023-05-30T18:51:53.652076Z"
    }
   },
   "outputs": [],
   "source": [
    "# class ScriptedDataTransform(torch.nn.Module):\n",
    "#     def __init__(self):   \n",
    "#         super().__init__()\n",
    "#         self.custom_transform = torch.nn.Sequential(\n",
    "#             torchvision.transforms.Resize((224, 224)), \n",
    "#             torchvision.transforms.RandomHorizontalFlip(),\n",
    "#             torchvision.transforms.ToTensor(), \n",
    "#             torchvision.transforms.Normalize(\n",
    "#                 mean=(0.485, 0.456, 0.406), \n",
    "#                 std=(0.229, 0.224, 0.225)\n",
    "#             )\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         output = self.custom_transform(x)\n",
    "#         return output\n",
    "\n",
    "# transformer = ScriptedDataTransform()\n",
    "# scripted_transformer = torch.jit.script(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "989ea244",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T18:51:53.795434Z",
     "start_time": "2023-05-30T18:51:53.791634Z"
    }
   },
   "outputs": [],
   "source": [
    "custom_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)), \n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(), \n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406), \n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1e61c2be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T18:53:08.576884Z",
     "start_time": "2023-05-30T18:53:07.789288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## need access\n",
    "# imagenet_dataset = torchvision.datasets.ImageNet(\n",
    "#     root='./data', \n",
    "#     split='train', \n",
    "#     transform=scripted_transformer, \n",
    "#     download=True\n",
    "# )\n",
    "\n",
    "# Download and load training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='data/', train=True, download=True, transform=custom_transform)\n",
    "trainset_loader = torch.utils.data.DataLoader(trainset, batch_size=5, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f9a45617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T18:53:09.838310Z",
     "start_time": "2023-05-30T18:53:09.187415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=custom_transform)\n",
    "testset_loader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4e46c865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T18:53:10.222312Z",
     "start_time": "2023-05-30T18:53:10.218663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of trainset = 50000\n",
      "size of trainloader = 10000\n",
      "size of testset = 10000\n",
      "size of testloader = 2000\n"
     ]
    }
   ],
   "source": [
    "print(f\"size of trainset = {len(trainset)}\")\n",
    "print(f\"size of trainloader = {len(trainset_loader)}\")\n",
    "print(f\"size of testset = {len(testset)}\")\n",
    "print(f\"size of testloader = {len(testset_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b650cfa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T18:53:10.920275Z",
     "start_time": "2023-05-30T18:53:10.894228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X shape: torch.Size([5, 3, 224, 224]) train Y: tensor([7, 1, 4, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "first_train_batch = next(iter(trainset_loader))\n",
    "train_x = first_train_batch[0]\n",
    "train_y  = first_train_batch[1]\n",
    "print(f'train X shape: {train_x.shape} train Y: {train_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea3610f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa86c27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T05:58:56.529720Z",
     "start_time": "2023-05-30T05:58:56.527222Z"
    }
   },
   "source": [
    "### Residual Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec67a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T06:02:45.417852Z",
     "start_time": "2023-05-30T06:02:45.413948Z"
    },
    "cell_style": "split"
   },
   "source": [
    "<img src=\"ResnetCNN.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd5805",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "* The image on the left shows a 34-layer ResNet\n",
    "* Some blocks are normal while some require **downsampling**\n",
    "\n",
    "Diagram explaination\n",
    "1. **7x7 conv, 64, /2** refers to `kernel size=7` `out_channels=64` and `stride=2`\n",
    "2. **pool, /2** refers to halfing H and W by using a `stride=2` and `max/avg pool kernel_size=2`\n",
    "3. **3x3 conv, 64** refers to  `kernel size=3` and `out_channels=64` and `stride=1 [default]`\n",
    "4. The **dotted lines** represent `downsampling` in order for us to makes the tensor shapes match while doing `x = x + residual` where `residual=downsample(x)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0585e328",
   "metadata": {},
   "source": [
    "**Excerpt from the Paper**\n",
    "* Residual Network. Based on the above plain network, we insert shortcut connections (Fig. 3, right) which turn the network into its counterpart residual version. The identity shortcuts (Eqn.(1)) can be directly used when the input and output are of the same dimensions (solid line shortcuts in Fig. 3). \n",
    "\n",
    "* When the dimensions increase (dotted line shortcuts in Fig. 3), we consider two options: \n",
    "    * (A) The shortcut still performs identity mapping, with extra zero entries padded for increasing dimensions. This option introduces no extra parameter; \n",
    "    * (B) The projection shortcut in Eqn.(2) is used to match dimensions (done by 1Ã—1 convolutions). For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d147cb96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T19:03:39.099856Z",
     "start_time": "2023-05-30T19:03:39.094384Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, downsample=False):\n",
    "        \"\"\"\n",
    "        The `downsample param is useful when the image size is reduced while chaining together the \n",
    "        residual blocks`\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.batchnorm = torch.nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        train_x, train_y = input_x\n",
    "\n",
    "        x = self.relu(self.batchnorm(self.conv(train_x)))  # purple rectangle from above diagram\n",
    "        \n",
    "        x = self.batchnorm(self.conv(x))\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(train_x)\n",
    "            \n",
    "        x += residual\n",
    "        \n",
    "        out = self.relu(x)\n",
    "        \n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "236cf79b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T19:03:39.915154Z",
     "start_time": "2023-05-30T19:03:39.833111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 224, 224]) torch.Size([5])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ResidualBlock' object has no attribute 'downsample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m residual_block \u001b[38;5;241m=\u001b[39m ResidualBlock(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mresidual_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_train_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cyml38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [94], line 24\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, input_x)\u001b[0m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(train_x)))\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m:\n\u001b[1;32m     25\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(train_x)\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cyml38/lib/python3.8/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResidualBlock' object has no attribute 'downsample'"
     ]
    }
   ],
   "source": [
    "residual_block = ResidualBlock(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=0)\n",
    "residual_block(first_train_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99c957",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "### Resnet \n",
    "* Here we'll utilize the ResidualBlock prepared above to build our Resnet\n",
    "* We'll first start with [see diagram above]\n",
    "    * A simple layer\n",
    "    * following by max pool\n",
    "    * followed by a bunch of residual blocks \n",
    "    * followed by fC layers\n",
    "    \n",
    "**Note**: We won't be constructing the original 150 layer reset. That's too big to train on PC. However, we'll try to emulate the 34-layer network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47bd26a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T03:33:15.695392Z",
     "start_time": "2023-05-31T03:33:15.691091Z"
    },
    "cell_style": "split"
   },
   "source": [
    "<img src=\"ResnetFullDiagram.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.Conv2d as conv2d\n",
    "import torch.nn.BatchNorm as batchnorm\n",
    "import torch.nn.ReLU as relu\n",
    "\n",
    "class Resnet(torch.nn.Module)\n",
    "    def __init__(self):\n",
    "        \n",
    "        # inital block\n",
    "        self.layer1 = relu(\n",
    "            batchnorm(\n",
    "                conv2d(\n",
    "                    in_channels=3, \n",
    "                    out_channels=16, \n",
    "                    kernel_size=3, \n",
    "                    stride=2\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.maxpool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # -------- ResidulalBlocks -------------\n",
    "        # Purple block of 3 skip connection blocks\n",
    "        self.block1 = self._make_block(num_layers_in_block=3, in_channels=64, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        # Green block of 1 downsample (dotted line) and 3 normal\n",
    "        self.block2 = self._make_block(num_layers_in_block=4, in_channels=64, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        # Red block of 1 downsample and 5 normal \n",
    "        self.block3 = ResidualBlock(in_channel=16, out_channels=32, kernel_size=3,stride=2) \n",
    "        \n",
    "        # Blue block of 1 downsample and 2 normal \n",
    "        self.block4 = ResidualBlock(in_channel=16, out_channels=32, kernel_size=3,stride=2) \n",
    "       \n",
    "        # final layers\n",
    "        self.avg_pool = torch.nn.AvgPool2d()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc = nn.Linear(in_features=64, out_features=10)\n",
    "        \n",
    "    def _make_block(\n",
    "        self,\n",
    "        num_layers_in_block,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        padding,\n",
    "        stride, downsample\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Helper function to create residual blocks on demand\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        \n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            print(\"We'll be connecting via dotted line to match the shapes\")\n",
    "            dotted_block = ResidualBlock(in_channels=in_channels, out_channels=out_channels, stride=2)\n",
    "            \n",
    "        layers.append(dotted_block)\n",
    "        for i in range(1, num_layers_in_block):\n",
    "            layers.append(ResidualBlock(in_channel=in_channels, out_channels=out_channels, kernel_size=3,stride=1))\n",
    "    \n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77985e59",
   "metadata": {},
   "source": [
    "## Read More\n",
    "* what is bottleneck\n",
    "* Advances in Resnets over the years\n",
    "* Uses of Resnets over the years - example Transformers. \n",
    "* Signifance of 1X1 convoluations for dimenstionality reduction across the channel axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75c92739",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T19:27:43.221843Z",
     "start_time": "2023-06-09T19:27:43.216284Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qg/3xfmq_jx7js8v4v0gtg73vg00000gq/T/ipykernel_16021/4152741321.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_t = torch.tensor(a, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10, 10])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "B = 2\n",
    "C = 3\n",
    "H = 10\n",
    "W = 10\n",
    "\n",
    "a = torch.randn(B, C, H, W).clone().detach()\n",
    "\n",
    "input_t = torch.tensor(a, dtype=torch.float32)\n",
    "input_t.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d99d49e",
   "metadata": {},
   "source": [
    "* As you can saee below, the entire Volume of shape C, K, K gets compressed into 1 scalar value. This is done in two step\n",
    "    * element wise matrix mul (also called as conv/correlation)\n",
    "    * squish the volume into a scalar by simply adding all the values up. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e010d000",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T19:27:43.714538Z",
     "start_time": "2023-06-09T19:27:43.707351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7579530477523804"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = torch.nn.Conv2d(in_channels=C, out_channels=8, stride=1, kernel_size=5, bias=False)\n",
    "kernel_weights = conv._parameters['weight'][0] # the weights are (3,(5,5))\n",
    "\n",
    "input_to_convolve = a[0][:, :5, :5]\n",
    "input_to_convolve.shape, kernel_weights.shape\n",
    "\n",
    "corr = torch.multiply(input_to_convolve, kernel_weights)\n",
    "torch.sum(corr).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e52840f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T19:27:44.362199Z",
     "start_time": "2023-06-09T19:27:44.357446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7579531073570251"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t = conv(input_t)\n",
    "first_channel_h_w = output_t[0][0]\n",
    "first_channel_h_w[0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee96c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe5260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321.179px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
